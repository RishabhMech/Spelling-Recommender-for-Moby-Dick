{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "python-text-mining",
      "graded_item_id": "r35En",
      "launcher_item_id": "tCVfW",
      "part_id": "NTVgL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Course4Ass2(MObyDIck).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3XtsmY8Vo-"
      },
      "source": [
        "# **Spelling Recommender for the Herman Melville novel, Moby Dick**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28u1N3AY8VpB"
      },
      "source": [
        "## **Analyzing Moby Dick**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWmCSiZG8VpF"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm30Numc8VpO",
        "outputId": "a61c1727-a1ca-4178-9e39-b8448a5e8ee5"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCMkIw7o8VpX"
      },
      "source": [
        "# If you would like to work with the raw text you can use 'moby_raw'\n",
        "with open('moby.txt', 'r') as f:\n",
        "    moby_raw = f.read()\n",
        "\n",
        "moby_tokens = nltk.word_tokenize(moby_raw)\n",
        "text1 = nltk.Text(moby_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nklYO-p28Vpe",
        "outputId": "c45837c9-f3bd-4a47-a36e-0b2567a92da9"
      },
      "source": [
        "type(moby_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkMXon9m8Vpk",
        "outputId": "d04cb918-bcd3-47dc-8011-4053b2a0b652"
      },
      "source": [
        "type(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.text.Text"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRCWFHZI8Vpq"
      },
      "source": [
        "How many tokens (words and punctuation symbols) are in text1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M8bYKws8Vpr",
        "outputId": "94b130b1-a636-429c-9d02-bc188a614986"
      },
      "source": [
        "def example_one():\n",
        "    \n",
        "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
        "\n",
        "example_one()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "254989"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmhOizcJ8Vpx"
      },
      "source": [
        "How many unique tokens (unique words and punctuation) does text1 have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGsLXTI8Vpx",
        "outputId": "38cb5b66-daab-47f0-c312-7883431bd4e7"
      },
      "source": [
        "def example_two():\n",
        "    \n",
        "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
        "\n",
        "example_two()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20755"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMOA4o98Vp2"
      },
      "source": [
        "After lemmatizing the verbs, how many unique tokens does text1 have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htAgKBWx8Vp3",
        "outputId": "41cfd131-e409-4edb-da1d-45b49b956583"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def example_three():\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
        "\n",
        "    return len(set(lemmatized))\n",
        "\n",
        "example_three()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2JZCcnY8Vp7"
      },
      "source": [
        "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SehkLy228Vp8",
        "outputId": "0988e63f-7616-4cc1-dfc9-0caca7d3a8d9"
      },
      "source": [
        "def answer_one():\n",
        "    \n",
        "    \n",
        "    return (len(set(nltk.word_tokenize(moby_raw))))/len(nltk.word_tokenize(moby_raw))\n",
        "\n",
        "answer_one()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08139566804842562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOzzH6r8VqB"
      },
      "source": [
        "What percentage of tokens is 'whale' or 'Whale'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB09fYna8VqC"
      },
      "source": [
        "def answer_two():\n",
        "    \n",
        "    \n",
        "    return len([w for w in nltk.word_tokenize(moby_raw) if w=='whale'or w=='Whale']) / len(nltk.word_tokenize(moby_raw))\n",
        "\n",
        "answer_two()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y9rHLcX8VqG"
      },
      "source": [
        "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7OvV87x8VqH"
      },
      "source": [
        "def answer_three():\n",
        "    tokens = nltk.word_tokenize(moby_raw)\n",
        "    dist = nltk.FreqDist(tokens)\n",
        "\n",
        "    return dist.most_common(20)\n",
        "\n",
        "answer_three()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiwyvll98VqL"
      },
      "source": [
        "What tokens have a length of greater than 5 and frequency of more than 150?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGPC-eay8VqM",
        "outputId": "f818461a-ff89-4df4-f42d-174b44b58a84"
      },
      "source": [
        "def answer_four():\n",
        "\n",
        "    tokens = nltk.word_tokenize(moby_raw)\n",
        "    dist = nltk.FreqDist(tokens)\n",
        "    \n",
        "    return [w for w,v in zip(dist.keys(), dist.values()) if len(w)>5 and v>150]\n",
        "\n",
        "answer_four()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['through',\n",
              " 'almost',\n",
              " 'whales',\n",
              " 'before',\n",
              " 'without',\n",
              " 'should',\n",
              " 'little',\n",
              " 'seemed',\n",
              " 'though',\n",
              " 'himself',\n",
              " 'Captain',\n",
              " 'Queequeg',\n",
              " 'Pequod',\n",
              " 'Starbuck']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5XLQ65H8VqR"
      },
      "source": [
        "Find the longest word in text1 and that word's length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_JjVtqI8VqR",
        "outputId": "36635f2f-eb7c-45c5-c387-fcea65bee2af"
      },
      "source": [
        "def answer_five():\n",
        "    tokens = nltk.word_tokenize(moby_raw)\n",
        "    tokens_uni = set(tokens)\n",
        "    longest_word = max(tokens_uni, key=len)\n",
        "    result = (longest_word, len(longest_word))\n",
        "    \n",
        "    return result\n",
        "\n",
        "answer_five()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"twelve-o'clock-at-night\", 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6p2_Ovo8VqV"
      },
      "source": [
        "What unique words have a frequency of more than 2000? What is their frequency?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4qLbW-Z8VqW",
        "outputId": "60e38cf9-dc3e-470d-c8fe-41dbd6d2d50a"
      },
      "source": [
        "def answer_six():\n",
        "    \n",
        "    tokens = nltk.word_tokenize(moby_raw)\n",
        "    dist = nltk.FreqDist(tokens)\n",
        "    return [(v,w) for w,v in zip(dist.keys(), dist.values()) if v>2000 and w.isalpha()]\n",
        "\n",
        "answer_six()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4545, 'a'),\n",
              " (4515, 'to'),\n",
              " (3908, 'in'),\n",
              " (6010, 'and'),\n",
              " (2097, 'I'),\n",
              " (2459, 'his'),\n",
              " (13715, 'the'),\n",
              " (6513, 'of'),\n",
              " (2196, 'it'),\n",
              " (2978, 'that')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3E09_88Vqa"
      },
      "source": [
        "What is the average number of tokens per sentence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIaISU6B8Vqb",
        "outputId": "615bc240-67f7-4843-de78-8e956da0c82b"
      },
      "source": [
        "def answer_seven():\n",
        "    c=0\n",
        "    sent_tokens = nltk.sent_tokenize(moby_raw)\n",
        "    for s in sent_tokens:\n",
        "        c=c+ len(nltk.word_tokenize(s))\n",
        "        \n",
        "    \n",
        "    return c/len(sent_tokens)\n",
        "\n",
        "answer_seven()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.881952902963864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWxDlqtc8Vqf"
      },
      "source": [
        "What are the 5 most frequent parts of speech in this text? What is their frequency?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mScOTHTI8Vqf",
        "outputId": "cdd1fe0b-42c4-4a09-e239-e4ffa249a7cc"
      },
      "source": [
        "def answer_eight():\n",
        "    \n",
        "    tokens = nltk.word_tokenize(moby_raw)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    dist = nltk.FreqDist([tag for (word, tag) in tags])\n",
        "    return dist.most_common(5)\n",
        "\n",
        "answer_eight()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQGNPXSq-8CJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eki2tNnJ8Vqj"
      },
      "source": [
        "## **Framing the Spelling Recommender for Moby Dick**\n",
        "\n",
        "The recommenders deliver recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCd1aQit8Vqk"
      },
      "source": [
        "from nltk.corpus import words\n",
        "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusBLY908Vqp"
      },
      "source": [
        "from nltk.corpus import words\n",
        "correct_spellings = words.words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDBgyTq8Vqv"
      },
      "source": [
        "This Recommender delivers the recommendations for the three default words provided above using the following distance metric:\n",
        "\n",
        "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjuYOg3y8Vqw"
      },
      "source": [
        "entries=['cormulent', 'incendenece', 'validrate']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0tJEW_ed8Vqz",
        "outputId": "3a53eb21-9a74-47d7-f457-4018eed548ab"
      },
      "source": [
        "def answer_nine(entries, gram_number):\n",
        "    \n",
        "    arr=[]\n",
        "    for entry in entries:\n",
        "        spellings = [s for s in correct_spellings if s.startswith(entry[0])]\n",
        "        df= pd.DataFrame(columns=['word', 'Dist_'])\n",
        "        dist_df1=[]\n",
        "        dist_df2=[]\n",
        "        for word in spellings:\n",
        "            dist_df1.append(word)\n",
        "            dist_df2.append(jaccard_distance(set(ngrams(entry, gram_number)), set(ngrams(word, gram_number))))\n",
        "        x= list(zip(dist_df2,dist_df1))\n",
        "        y= min(x)\n",
        "        arr.append(y[1])\n",
        "    return arr\n",
        "answer_nine(entries,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corpulent', 'indecence', 'validate']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxHHMMz18Vq4"
      },
      "source": [
        "This recommender delivers the recommendations for the three default words provided above using the following distance metric:\n",
        "\n",
        "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijTONfNz8Vq5",
        "outputId": "0e069dbd-28bb-45aa-9a4c-64ad2c376e4e"
      },
      "source": [
        "def answer_ten():\n",
        "\n",
        "    return answer_nine(entries,4)\n",
        "    \n",
        "answer_ten()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cormus', 'incendiary', 'valid']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_mszrSe8Vq-"
      },
      "source": [
        "This recommender delivers the recommendations for the three default words provided above using the following distance metric:\n",
        "\n",
        "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig_CwRzL8VrA",
        "outputId": "026442fb-e11a-45d5-ec2b-b1c0b92bc9b3"
      },
      "source": [
        "def answer_eleven(entries):\n",
        "\n",
        "    arr=[]\n",
        "    for entry in entries:\n",
        "        spellings = [s for s in correct_spellings if s.startswith(entry[0])]\n",
        "        df= pd.DataFrame(columns=['word', 'Dist_'])\n",
        "        dist_df1=[]\n",
        "        dist_df2=[]\n",
        "        for word in spellings:\n",
        "            dist_df1.append(word)\n",
        "            dist_df2.append(edit_distance(entry, word))\n",
        "        x= list(zip(dist_df2,dist_df1))\n",
        "        y= min(x)\n",
        "        arr.append(y[1])\n",
        "    return arr\n",
        "    \n",
        "answer_eleven(entries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corpulent', 'intendence', 'validate']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}